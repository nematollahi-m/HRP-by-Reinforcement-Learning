{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FarmEnv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the gym environment for the pruning problem"
      ],
      "metadata": {
        "id": "gcmLHryppdwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meqvN-wNSu2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22517194-4ee1-47db-88bb-8e02244309d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.5)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.10.0) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2022.1)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 684 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /tensorflow-1.15.2/python3.7 (from keras-rl2) (1.15.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow->keras-rl2) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow->keras-rl2) (1.15.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow->keras-rl2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=6f47d053673b9d0695376e8a895f9c456fe1c7c2d8ae46832ca61bbc3beed78b\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, keras-rl2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install keras-rl2\n",
        "\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete,Box\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# model parameters\n",
        "\n",
        "MAX_ALLOWED_WORKER = 10\n",
        "BUDGET = 5000\n",
        "PLANTS = 10000\n",
        "WAGE = 35\n",
        "PRODUCTIVITY = 700\n",
        "HIRE_COST = 100\n",
        "FIRE_COST = 0\n",
        "PRUNE_LENGTH = 3\n",
        "PRUNE_PROFIT = 3\n",
        "\n",
        "\n",
        "action_size = (MAX_ALLOWED_WORKER + 1) * (MAX_ALLOWED_WORKER + 1)\n",
        "input_shape = 3\n",
        "alpha = 0.6\n",
        "beta = 0.4\n",
        "\n",
        "\n",
        "class FarmEnv(Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # state = (number of hired workers, remaining budget , remaining plants)\n",
        "        self.state = np.array([0,BUDGET,PLANTS])\n",
        "        self.action_space = Discrete((MAX_ALLOWED_WORKER + 1) * (MAX_ALLOWED_WORKER + 1))\n",
        "        self.observation_space = Box(low = np.array([0,0,0]), high = np.array([+MAX_ALLOWED_WORKER, +BUDGET, +PLANTS]))\n",
        "        self.prune_len = PRUNE_LENGTH\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.prune_len -= 1\n",
        "        info = {}\n",
        "        done = False\n",
        "\n",
        "        mapping = tuple(np.ndindex((MAX_ALLOWED_WORKER + 1, MAX_ALLOWED_WORKER + 1)))\n",
        "        new_action = mapping[action]\n",
        "\n",
        "        current_employee = self.state[0]\n",
        "        cost = BUDGET - self.state[1]\n",
        "        pruned_plants = PLANTS - self.state[2]\n",
        "\n",
        "\n",
        "        hired = new_action[0]\n",
        "        fired = new_action[1]\n",
        "\n",
        "        reward = 0.0\n",
        "\n",
        "        if (current_employee + hired) > MAX_ALLOWED_WORKER:\n",
        "            reward = -5000.0\n",
        "            return self.state, reward, done, info\n",
        "        else:\n",
        "            current_employee += hired\n",
        "\n",
        "        cost += (hired * HIRE_COST)\n",
        "        cost += (current_employee * WAGE)\n",
        "        cost += (fired * FIRE_COST)\n",
        "\n",
        "        pruned_plants += current_employee * round(np.random.normal(PRODUCTIVITY, 1))\n",
        "\n",
        "        if current_employee - fired < 0:\n",
        "            reward = -5000.0\n",
        "            return self.state, reward, done, info\n",
        "        else:\n",
        "            current_employee = current_employee - fired\n",
        "\n",
        "        if cost > BUDGET:\n",
        "            reward -= 5000.0\n",
        "            done = True\n",
        "\n",
        "        if pruned_plants >= PLANTS:\n",
        "            reward += 5000.0\n",
        "            done = True\n",
        "\n",
        "        if self.prune_len <= 0:\n",
        "            done = True\n",
        "\n",
        "        reward += ((alpha * (pruned_plants * PRUNE_PROFIT)) - (beta * cost) )\n",
        "\n",
        "        self.state = np.array([current_employee, BUDGET - cost, PLANTS - pruned_plants])\n",
        "        # self.state = current_employee * (BUDGET - cost) * (PLANTS - pruned_plants)\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        # self.state = np.asarray([random.randint(0, MAX_ALLOWED_WORKER), random.randint(0, BUDGET),\n",
        "        #                           random.randint(0, PLANTS)])\n",
        "        self.state = np.asarray([0,BUDGET,PLANTS])\n",
        "        self.prune_len = PRUNE_LENGTH\n",
        "        return self.state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the environment"
      ],
      "metadata": {
        "id": "EufubIbQptrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = FarmEnv()\n",
        "actions = env.action_space.n\n",
        "states = env.observation_space.shape\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"The initial observation is {}\".format(obs))\n",
        "\n",
        "# Sample a random action from the entire action space\n",
        "random_action = env.action_space.sample()\n",
        "\n",
        "\n",
        "# # Take the action and get the new observation space\n",
        "new_obs, reward, done, info = env.step(random_action)\n",
        "print(\"The new observation is {}\".format(new_obs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gc35QSmS59_",
        "outputId": "a923e951-d807-4e49-ea2b-7aa30bf7f375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The initial observation is [    0  5000 10000]\n",
            "The new observation is [   0 4055 5100]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://stable-baselines.readthedocs.io/en/master/guide/callbacks.html"
      ],
      "metadata": {
        "id": "E9Xzy51Up1ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines.common.callbacks import BaseCallback\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu5mWqZ_nt-S",
        "outputId": "bb27c696-5ff7-450c-c3b8-0832fdb462b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the gym envirnment and initializing log to track the reward. "
      ],
      "metadata": {
        "id": "1b4sLL8qpDyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines.common.env_checker import check_env\n",
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "import os\n",
        "from stable_baselines.bench import Monitor\n",
        "\n",
        "# making sure the environment is correct based on Gym definition. \n",
        "print(check_env(env, warn=True))\n",
        "\n",
        "# traking rewards\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "env = Monitor(env, log_dir)\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eS1iAtSS-HH",
        "outputId": "98f22599-d257-4242-82e5-0667a13c3db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model \n",
        "Source: https://stable-baselines.readthedocs.io/en/master/"
      ],
      "metadata": {
        "id": "uMdlyOoDo22Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "time_steps = 25000\n",
        "model = DQN('MlpPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=time_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBiXeQMhmVg1",
        "outputId": "4b87262b-b4aa-4797-a008-841964f5f617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 82       |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 1.59e+04 |\n",
            "| steps                   | 440      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 65       |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 9.54e+03 |\n",
            "| steps                   | 874      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 39       |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 7.42e+03 |\n",
            "| steps                   | 1543     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 26       |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 2.68e+04 |\n",
            "| steps                   | 1866     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 16       |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 3.47e+04 |\n",
            "| steps                   | 2132     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 6        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 3.53e+04 |\n",
            "| steps                   | 2393     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 3.8e+04  |\n",
            "| steps                   | 2598     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 3.43e+04 |\n",
            "| steps                   | 2894     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 3.95e+04 |\n",
            "| steps                   | 3097     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 3.92e+04 |\n",
            "| steps                   | 3300     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1100     |\n",
            "| mean 100 episode reward | 4.4e+04  |\n",
            "| steps                   | 3535     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1200     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 3747     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1300     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 3964     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1400     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 4179     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1500     |\n",
            "| mean 100 episode reward | 4.28e+04 |\n",
            "| steps                   | 4406     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1600     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 4620     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1700     |\n",
            "| mean 100 episode reward | 3.24e+04 |\n",
            "| steps                   | 4974     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1800     |\n",
            "| mean 100 episode reward | 4.03e+04 |\n",
            "| steps                   | 5184     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1900     |\n",
            "| mean 100 episode reward | 3.88e+04 |\n",
            "| steps                   | 5393     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2000     |\n",
            "| mean 100 episode reward | 3.93e+04 |\n",
            "| steps                   | 5595     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2100     |\n",
            "| mean 100 episode reward | 3.52e+04 |\n",
            "| steps                   | 5890     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2200     |\n",
            "| mean 100 episode reward | 4.01e+04 |\n",
            "| steps                   | 6097     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2300     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 6310     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2400     |\n",
            "| mean 100 episode reward | 3.75e+04 |\n",
            "| steps                   | 6572     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2500     |\n",
            "| mean 100 episode reward | 3.9e+04  |\n",
            "| steps                   | 6777     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2600     |\n",
            "| mean 100 episode reward | 3.85e+04 |\n",
            "| steps                   | 7000     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2700     |\n",
            "| mean 100 episode reward | 3.94e+04 |\n",
            "| steps                   | 7205     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2800     |\n",
            "| mean 100 episode reward | 3.87e+04 |\n",
            "| steps                   | 7451     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2900     |\n",
            "| mean 100 episode reward | 3.48e+04 |\n",
            "| steps                   | 7767     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3000     |\n",
            "| mean 100 episode reward | 3.97e+04 |\n",
            "| steps                   | 7972     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3100     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 8178     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3200     |\n",
            "| mean 100 episode reward | 4.02e+04 |\n",
            "| steps                   | 8388     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3300     |\n",
            "| mean 100 episode reward | 3.95e+04 |\n",
            "| steps                   | 8592     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3400     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 8798     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3500     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 9010     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3600     |\n",
            "| mean 100 episode reward | 4e+04    |\n",
            "| steps                   | 9214     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3700     |\n",
            "| mean 100 episode reward | 3.97e+04 |\n",
            "| steps                   | 9418     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3800     |\n",
            "| mean 100 episode reward | 3.94e+04 |\n",
            "| steps                   | 9621     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3900     |\n",
            "| mean 100 episode reward | 3.35e+04 |\n",
            "| steps                   | 9955     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4000     |\n",
            "| mean 100 episode reward | 3.96e+04 |\n",
            "| steps                   | 10157    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4100     |\n",
            "| mean 100 episode reward | 3.96e+04 |\n",
            "| steps                   | 10365    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4200     |\n",
            "| mean 100 episode reward | 3.96e+04 |\n",
            "| steps                   | 10566    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4300     |\n",
            "| mean 100 episode reward | 3.64e+04 |\n",
            "| steps                   | 10782    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4400     |\n",
            "| mean 100 episode reward | 3.24e+04 |\n",
            "| steps                   | 11007    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4500     |\n",
            "| mean 100 episode reward | 3.98e+04 |\n",
            "| steps                   | 11209    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4600     |\n",
            "| mean 100 episode reward | 3.71e+04 |\n",
            "| steps                   | 11469    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4700     |\n",
            "| mean 100 episode reward | 4.01e+04 |\n",
            "| steps                   | 11674    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4800     |\n",
            "| mean 100 episode reward | 3.97e+04 |\n",
            "| steps                   | 11878    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4900     |\n",
            "| mean 100 episode reward | 3.92e+04 |\n",
            "| steps                   | 12087    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5000     |\n",
            "| mean 100 episode reward | 3.98e+04 |\n",
            "| steps                   | 12310    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5100     |\n",
            "| mean 100 episode reward | 4.08e+04 |\n",
            "| steps                   | 12512    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5200     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 12716    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5300     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 12920    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5400     |\n",
            "| mean 100 episode reward | 4.08e+04 |\n",
            "| steps                   | 13124    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5500     |\n",
            "| mean 100 episode reward | 3.72e+04 |\n",
            "| steps                   | 13338    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5600     |\n",
            "| mean 100 episode reward | 3.79e+04 |\n",
            "| steps                   | 13551    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5700     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 13756    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5800     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 13960    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5900     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 14164    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6000     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 14368    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6100     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 14573    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6200     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 14774    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6300     |\n",
            "| mean 100 episode reward | 3.95e+04 |\n",
            "| steps                   | 14979    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6400     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 15186    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6500     |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 15387    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6600     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 15593    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6700     |\n",
            "| mean 100 episode reward | 4.16e+04 |\n",
            "| steps                   | 15794    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6800     |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 15997    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6900     |\n",
            "| mean 100 episode reward | 3.72e+04 |\n",
            "| steps                   | 16218    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7000     |\n",
            "| mean 100 episode reward | 3.54e+04 |\n",
            "| steps                   | 16438    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7100     |\n",
            "| mean 100 episode reward | 3.99e+04 |\n",
            "| steps                   | 16646    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7200     |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 16849    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7300     |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 17051    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7400     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 17253    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7500     |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 17457    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7600     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 17662    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7700     |\n",
            "| mean 100 episode reward | 3.68e+04 |\n",
            "| steps                   | 17964    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7800     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 18170    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7900     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 18374    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8000     |\n",
            "| mean 100 episode reward | 3.97e+04 |\n",
            "| steps                   | 18610    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8100     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 18814    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8200     |\n",
            "| mean 100 episode reward | 4.02e+04 |\n",
            "| steps                   | 19042    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8300     |\n",
            "| mean 100 episode reward | 3.85e+04 |\n",
            "| steps                   | 19302    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8400     |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 19505    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8500     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 19710    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8600     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 19917    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8700     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 20120    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8800     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 20322    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8900     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 20524    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9000     |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 20728    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9100     |\n",
            "| mean 100 episode reward | 4.02e+04 |\n",
            "| steps                   | 20936    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9200     |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 21138    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9300     |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 21339    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9400     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 21543    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9500     |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 21744    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9600     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 21948    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9700     |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 22150    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9800     |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 22351    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9900     |\n",
            "| mean 100 episode reward | 4.08e+04 |\n",
            "| steps                   | 22555    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10000    |\n",
            "| mean 100 episode reward | 3.92e+04 |\n",
            "| steps                   | 22766    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10100    |\n",
            "| mean 100 episode reward | 3.66e+04 |\n",
            "| steps                   | 23044    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10200    |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 23264    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10300    |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 23465    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10400    |\n",
            "| mean 100 episode reward | 4.13e+04 |\n",
            "| steps                   | 23670    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10500    |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 23871    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10600    |\n",
            "| mean 100 episode reward | 4.14e+04 |\n",
            "| steps                   | 24077    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10700    |\n",
            "| mean 100 episode reward | 3.14e+04 |\n",
            "| steps                   | 24387    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10800    |\n",
            "| mean 100 episode reward | 3.64e+04 |\n",
            "| steps                   | 24605    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10900    |\n",
            "| mean 100 episode reward | 4.15e+04 |\n",
            "| steps                   | 24807    |\n",
            "--------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.deepq.dqn.DQN at 0x7fecd21d53d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "4RndZ8V8mwf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "track_reward = []\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    track_reward.append(reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPdlrujBmwBt",
        "outputId": "bc01a7b7-348d-4497-c26c-642441a1634b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  110\n",
            "obs= [  10 3650 3000] reward= 12060.0 done= False\n",
            "Step 2\n",
            "Action:  1\n",
            "obs= [    9  3300 -4010] reward= 29538.0 done= True\n",
            "Goal reached! reward= 29538.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to plot the rewards"
      ],
      "metadata": {
        "id": "rPjcZ9oypTpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines import results_plotter\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "\n",
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "    \n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=50)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "EW5WNgbTm4Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rA5FOhjUpM5c",
        "outputId": "c1714676-8e84-4890-db0f-484537abfdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for mapping actions numbers to array. (Hire,Fire)"
      ],
      "metadata": {
        "id": "Vtx0ILoKpXZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = tuple(np.ndindex((MAX_ALLOWED_WORKER + 1, MAX_ALLOWED_WORKER + 1)))\n",
        "\n",
        "new_action = mapping[1]\n",
        "\n",
        "print(new_action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJTmXYaDug0k",
        "outputId": "46e4b3c6-e3c1-4543-f0b2-9e6e1ba49ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 1)\n"
          ]
        }
      ]
    }
  ]
}