{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FarmEnv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meqvN-wNSu2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac4cba8-8492-4c23-c967-3929b9ab707e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.10.0) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2022.1)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 690 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /tensorflow-1.15.2/python3.7 (from keras-rl2) (1.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow->keras-rl2) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow->keras-rl2) (1.0.8)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow->keras-rl2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=e83cd394d9ee65a88cd8f32cc9d38ee4b8ad27d1319718cb84debf9bb88a3cf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, keras-rl2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install keras-rl2\n",
        "\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete,Box\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# model parameters\n",
        "MAX_ALLOWED_WORKER = 10\n",
        "BUDGET = 5000\n",
        "PLANTS = 10000\n",
        "WAGE = 35\n",
        "PRODUCTIVITY = 700\n",
        "HIRE_COST = 100\n",
        "FIRE_COST = 0\n",
        "PRUNE_LENGTH = 3\n",
        "PRUNE_PROFIT = 3\n",
        "action_size = (MAX_ALLOWED_WORKER + 1) * (MAX_ALLOWED_WORKER + 1)\n",
        "input_shape = 3\n",
        "alpha = 0.6\n",
        "beta = 0.4\n",
        "\n",
        "\n",
        "class FarmEnv(Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # state = (number of hired workers, remaining budget , remaining plants)\n",
        "        self.state = np.array([0,BUDGET,PLANTS])\n",
        "        self.action_space = Discrete((MAX_ALLOWED_WORKER + 1) * (MAX_ALLOWED_WORKER + 1))\n",
        "        self.observation_space = Box(low = np.array([0,0,0]), high = np.array([+MAX_ALLOWED_WORKER, +BUDGET, +PLANTS]))\n",
        "        self.prune_len = PRUNE_LENGTH\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.prune_len -= 1\n",
        "        info = {}\n",
        "        done = False\n",
        "\n",
        "        mapping = tuple(np.ndindex((MAX_ALLOWED_WORKER + 1, MAX_ALLOWED_WORKER + 1)))\n",
        "        new_action = mapping[action]\n",
        "\n",
        "        current_employee = self.state[0]\n",
        "        cost = BUDGET - self.state[1]\n",
        "        pruned_plants = PLANTS - self.state[2]\n",
        "\n",
        "\n",
        "        hired = new_action[0]\n",
        "        fired = new_action[1]\n",
        "\n",
        "        reward = 0.0\n",
        "\n",
        "        if (current_employee + hired) > MAX_ALLOWED_WORKER:\n",
        "            reward = -5000.0\n",
        "            return self.state, reward, done, info\n",
        "        else:\n",
        "            current_employee += hired\n",
        "\n",
        "        cost += (hired * HIRE_COST)\n",
        "        cost += (current_employee * WAGE)\n",
        "        cost += (fired * FIRE_COST)\n",
        "\n",
        "        pruned_plants += current_employee * round(np.random.normal(PRODUCTIVITY, 1))\n",
        "\n",
        "        if current_employee - fired < 0:\n",
        "            reward = -5000.0\n",
        "            return self.state, reward, done, info\n",
        "        else:\n",
        "            current_employee = current_employee - fired\n",
        "\n",
        "        if cost > BUDGET:\n",
        "            reward -= 5000.0\n",
        "            done = True\n",
        "\n",
        "        if pruned_plants >= PLANTS:\n",
        "            reward += 5000.0\n",
        "            done = True\n",
        "\n",
        "        if self.prune_len <= 0:\n",
        "            done = True\n",
        "\n",
        "        reward += ((alpha * (pruned_plants * PRUNE_PROFIT)) - (beta * cost) )\n",
        "\n",
        "        self.state = np.array([current_employee, BUDGET - cost, PLANTS - pruned_plants])\n",
        "        # self.state = current_employee * (BUDGET - cost) * (PLANTS - pruned_plants)\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        # self.state = np.asarray([random.randint(0, MAX_ALLOWED_WORKER), random.randint(0, BUDGET),\n",
        "        #                           random.randint(0, PLANTS)])\n",
        "        self.state = np.asarray([0,BUDGET,PLANTS])\n",
        "        self.prune_len = PRUNE_LENGTH\n",
        "        return self.state\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = FarmEnv()\n",
        "actions = env.action_space.n\n",
        "states = env.observation_space.shape\n",
        "\n",
        "obs = env.reset()\n",
        "print(\"The initial observation is {}\".format(obs))\n",
        "\n",
        "# Sample a random action from the entire action space\n",
        "random_action = env.action_space.sample()\n",
        "\n",
        "\n",
        "# # Take the action and get the new observation space\n",
        "new_obs, reward, done, info = env.step(random_action)\n",
        "print(\"The new observation is {}\".format(new_obs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gc35QSmS59_",
        "outputId": "9b143184-4676-4889-9295-8b4c3f084970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The initial observation is [    0  5000 10000]\n",
            "The new observation is [    0  5000 10000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines.common.callbacks import BaseCallback\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu5mWqZ_nt-S",
        "outputId": "d2a07f2a-5146-4141-bf86-7545f223941d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines.common.env_checker import check_env\n",
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "import os\n",
        "from stable_baselines.bench import Monitor\n",
        "\n",
        "print(check_env(env, warn=True))\n",
        "\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "env = Monitor(env, log_dir)\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eS1iAtSS-HH",
        "outputId": "a9d13c9a-43b2-4ffb-ca7f-9a4a3ab0819d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_steps = 25000\n",
        "model = DQN('MlpPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=time_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBiXeQMhmVg1",
        "outputId": "f8d99ef8-fa93-4ad4-c527-dc4b79b0b721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 84       |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 1.69e+04 |\n",
            "| steps                   | 385      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 67       |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 1.36e+04 |\n",
            "| steps                   | 823      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 51       |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 1.43e+04 |\n",
            "| steps                   | 1231     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 32       |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 2.1e+04  |\n",
            "| steps                   | 1722     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 21       |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 3.48e+04 |\n",
            "| steps                   | 1993     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 9        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 3.29e+04 |\n",
            "| steps                   | 2303     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 4.01e+04 |\n",
            "| steps                   | 2510     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 2712     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 2913     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 3.81e+04 |\n",
            "| steps                   | 3170     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1100     |\n",
            "| mean 100 episode reward | 2.64e+04 |\n",
            "| steps                   | 3633     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1200     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 3833     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1300     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 4034     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1400     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 4237     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1500     |\n",
            "| mean 100 episode reward | 4.02e+04 |\n",
            "| steps                   | 4443     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1600     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 4646     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1700     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 4852     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1800     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 5055     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1900     |\n",
            "| mean 100 episode reward | 3.91e+04 |\n",
            "| steps                   | 5278     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2000     |\n",
            "| mean 100 episode reward | 6.68e+03 |\n",
            "| steps                   | 6144     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| % time spent exploring  | 2         |\n",
            "| episodes                | 2100      |\n",
            "| mean 100 episode reward | -2.42e+03 |\n",
            "| steps                   | 7212      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2200     |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 7415     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2300     |\n",
            "| mean 100 episode reward | 4e+04    |\n",
            "| steps                   | 7625     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2400     |\n",
            "| mean 100 episode reward | 3.91e+04 |\n",
            "| steps                   | 7858     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2500     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 8060     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2600     |\n",
            "| mean 100 episode reward | 3.88e+04 |\n",
            "| steps                   | 8299     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2700     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 8502     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2800     |\n",
            "| mean 100 episode reward | 4.03e+04 |\n",
            "| steps                   | 8706     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2900     |\n",
            "| mean 100 episode reward | 4.03e+04 |\n",
            "| steps                   | 8913     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3000     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 9119     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3100     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 9326     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3200     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 9530     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3300     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 9733     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3400     |\n",
            "| mean 100 episode reward | 3.67e+04 |\n",
            "| steps                   | 10014    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3500     |\n",
            "| mean 100 episode reward | 2.65e+04 |\n",
            "| steps                   | 10253    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3600     |\n",
            "| mean 100 episode reward | 2.37e+04 |\n",
            "| steps                   | 10499    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3700     |\n",
            "| mean 100 episode reward | 3.8e+04  |\n",
            "| steps                   | 10705    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3800     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 10907    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3900     |\n",
            "| mean 100 episode reward | 3.95e+04 |\n",
            "| steps                   | 11113    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4000     |\n",
            "| mean 100 episode reward | 3.9e+04  |\n",
            "| steps                   | 11315    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4100     |\n",
            "| mean 100 episode reward | 3.96e+04 |\n",
            "| steps                   | 11523    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4200     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 11724    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4300     |\n",
            "| mean 100 episode reward | 4.01e+04 |\n",
            "| steps                   | 11928    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4400     |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 12130    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4500     |\n",
            "| mean 100 episode reward | 3.85e+04 |\n",
            "| steps                   | 12332    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4600     |\n",
            "| mean 100 episode reward | 4.01e+04 |\n",
            "| steps                   | 12535    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4700     |\n",
            "| mean 100 episode reward | 3.92e+04 |\n",
            "| steps                   | 12737    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4800     |\n",
            "| mean 100 episode reward | 3.87e+04 |\n",
            "| steps                   | 12942    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4900     |\n",
            "| mean 100 episode reward | 3.91e+04 |\n",
            "| steps                   | 13144    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5000     |\n",
            "| mean 100 episode reward | 3.94e+04 |\n",
            "| steps                   | 13344    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5100     |\n",
            "| mean 100 episode reward | 3.92e+04 |\n",
            "| steps                   | 13550    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5200     |\n",
            "| mean 100 episode reward | 4e+04    |\n",
            "| steps                   | 13756    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5300     |\n",
            "| mean 100 episode reward | 3.97e+04 |\n",
            "| steps                   | 13961    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5400     |\n",
            "| mean 100 episode reward | 3.94e+04 |\n",
            "| steps                   | 14161    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5500     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 14363    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5600     |\n",
            "| mean 100 episode reward | 3.99e+04 |\n",
            "| steps                   | 14566    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5700     |\n",
            "| mean 100 episode reward | 3.99e+04 |\n",
            "| steps                   | 14770    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5800     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 14972    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5900     |\n",
            "| mean 100 episode reward | 3.85e+04 |\n",
            "| steps                   | 15214    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6000     |\n",
            "| mean 100 episode reward | 3.95e+04 |\n",
            "| steps                   | 15417    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6100     |\n",
            "| mean 100 episode reward | 3.8e+04  |\n",
            "| steps                   | 15628    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6200     |\n",
            "| mean 100 episode reward | 2.92e+04 |\n",
            "| steps                   | 15864    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6300     |\n",
            "| mean 100 episode reward | 3.49e+04 |\n",
            "| steps                   | 16084    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6400     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 16287    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6500     |\n",
            "| mean 100 episode reward | 4e+04    |\n",
            "| steps                   | 16497    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6600     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 16700    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6700     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 16905    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6800     |\n",
            "| mean 100 episode reward | 4.04e+04 |\n",
            "| steps                   | 17108    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6900     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 17309    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7000     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 17514    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7100     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 17718    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7200     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 17923    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7300     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 18128    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7400     |\n",
            "| mean 100 episode reward | 4.07e+04 |\n",
            "| steps                   | 18332    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7500     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 18535    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7600     |\n",
            "| mean 100 episode reward | 4.06e+04 |\n",
            "| steps                   | 18738    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7700     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 18941    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7800     |\n",
            "| mean 100 episode reward | 3.76e+04 |\n",
            "| steps                   | 19156    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7900     |\n",
            "| mean 100 episode reward | 3.57e+04 |\n",
            "| steps                   | 19373    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8000     |\n",
            "| mean 100 episode reward | 3.89e+04 |\n",
            "| steps                   | 19579    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8100     |\n",
            "| mean 100 episode reward | 4.08e+04 |\n",
            "| steps                   | 19781    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8200     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 19987    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8300     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 20192    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8400     |\n",
            "| mean 100 episode reward | 3.8e+04  |\n",
            "| steps                   | 20404    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8500     |\n",
            "| mean 100 episode reward | 3.84e+04 |\n",
            "| steps                   | 20612    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8600     |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 20819    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8700     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 21023    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8800     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 21227    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8900     |\n",
            "| mean 100 episode reward | 3.94e+04 |\n",
            "| steps                   | 21460    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9000     |\n",
            "| mean 100 episode reward | 4.11e+04 |\n",
            "| steps                   | 21663    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9100     |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 21867    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9200     |\n",
            "| mean 100 episode reward | 4.08e+04 |\n",
            "| steps                   | 22073    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9300     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 22280    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9400     |\n",
            "| mean 100 episode reward | 4.05e+04 |\n",
            "| steps                   | 22487    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9500     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 22691    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9600     |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 22894    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9700     |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 23097    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9800     |\n",
            "| mean 100 episode reward | 3.99e+04 |\n",
            "| steps                   | 23314    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9900     |\n",
            "| mean 100 episode reward | 4.1e+04  |\n",
            "| steps                   | 23519    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10000    |\n",
            "| mean 100 episode reward | 4.09e+04 |\n",
            "| steps                   | 23726    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10100    |\n",
            "| mean 100 episode reward | 4.02e+04 |\n",
            "| steps                   | 23930    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10200    |\n",
            "| mean 100 episode reward | 3.63e+04 |\n",
            "| steps                   | 24229    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10300    |\n",
            "| mean 100 episode reward | 4.12e+04 |\n",
            "| steps                   | 24431    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10400    |\n",
            "| mean 100 episode reward | 3.59e+04 |\n",
            "| steps                   | 24647    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10500    |\n",
            "| mean 100 episode reward | 3.24e+04 |\n",
            "| steps                   | 24873    |\n",
            "--------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.deepq.dqn.DQN at 0x7f3aae28ab90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "4RndZ8V8mwf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "track_reward = []\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    track_reward.append(reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPdlrujBmwBt",
        "outputId": "e22b5c80-6b77-4008-a27d-b741bb75aab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  111\n",
            "obs= [   9 3650 3000] reward= 12060.0 done= False\n",
            "Step 2\n",
            "Action:  1\n",
            "obs= [    8  3335 -3300] reward= 28274.0 done= True\n",
            "Goal reached! reward= 28274.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines import results_plotter\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "\n",
        "def moving_average(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "    \n",
        "def plot_results(log_folder, title='Learning Curve'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = moving_average(y, window=50)\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "EW5WNgbTm4Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rA5FOhjUpM5c",
        "outputId": "60577b50-7c29-4359-c253-0fd3e00a78cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = tuple(np.ndindex((MAX_ALLOWED_WORKER + 1, MAX_ALLOWED_WORKER + 1)))\n",
        "\n",
        "new_action = mapping[1]\n",
        "\n",
        "print(new_action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJTmXYaDug0k",
        "outputId": "271ba52c-687f-4a21-e696-331afebba1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 1)\n"
          ]
        }
      ]
    }
  ]
}